---
title: "Final assignment"
author: "Veronica Capuccio"
date: "04 August 2017"
output: html_document
---

##Final assignment

##Descriptive statistics and cleaning data


```{r}
library(ggplot2)
library(caret)
library(corrplot)

#Set directory
rm(list=ls())
setwd("C:\\Users\\Veronica\\Desktop\\Coursera\\Machine learning\\week4")

#Load data
training<-read.csv("pml-training.csv", na.strings=c("","NA"))
testing<-read.csv("pml-testing.csv", na.strings=c("","NA"))
```


The response variable is a qualitative variable with 5 categories.
There are 159 possible predictors.

I convert some factor variables in to numeric variables because they have been erroneously declared as factors but their nature is numeric.
I excluded class, username, cvtd_timestamp and new_window(yes/no) variables.

```{r, warning=FALSE}
colfactor<-sapply(training, is.factor)
colfactor[c(2,5,6,160)]<-FALSE

training[colfactor]<-lapply(training[colfactor], function(x) as.numeric(as.character(x)))
training<-as.data.frame(training)
```


I calculate the percentage of missing value for each variable

```{r}
count<-apply(training, 2, function(x) sum(is.na(x)))
NApercentage<-(count/dim(training)[1]) * 100
table(NApercentage)

posdropvar<-which(NApercentage>97)
length(posdropvar) 
```

There are 100 predictors with a percentage of missing values > 97%. 
I remove the first 7 variables because they are user details and the features with missing values. I have 52 predictors.

```{r}
training2<-training[,-posdropvar]
training2<-training2[,-c(1:7)]
```


I create two partitions: 70% train and 30% test from original training data.

```{r}
set.seed(3523)
inTrain<-createDataPartition(y=training2$classe, p=0.7, list=F)
train<-training2[inTrain,]
test<-training2[-inTrain,]
```


##Linear discriminant analysis 


```{r}
modlda<-train(classe~., data=train, method="lda") 
pred<-predict(modlda, test)
confusionMatrix(pred, test$classe)$overall[1]
```

Linear discriminant analysis has an accuracy equal to 0.7016 on test set. 


##Decision tree

I try to use a decision tree.

```{r}
modDT<-train(classe~.,data=train, method="rpart")
plot(modDT$finalModel,margin=0.2,uniform=T,branch=0.1)
text(modDT$finalModel,use.n=T,cex=0.6)

pred<-predict(modDT, test)
confusionMatrix(pred, test$classe)$overall[1] #0.4961
```

Decision tree has an accuracy equal to 0.4961. It is lower than lda method.

##Random forest
I try to adapt a random forest.

```{r}
fitControl <- trainControl(method = 'cv', number = 3)
modelRF <- train(classe ~.,data = train,
                  trControl = fitControl,method = 'rf',ntree = 100)
pred<-predict(modelRF, test)
confusionMatrix(pred, test$classe)$overall[1] #0.9918

```

##Prediction 

The final model is modRF with accuracy equal to 0.9918 on test set.

I predict the 20 new observations on testing set:


```{r}
predict(modelRF, testing)
```



